{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53720842",
   "metadata": {},
   "source": [
    "# Tag classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d88612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # TODO: Implement text preprocessing logic\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99681cbd",
   "metadata": {},
   "source": [
    "## Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"synthetic_dataset.csv\")\n",
    "df['tags'] = df['tags'].apply(lambda x: [tag.strip().lower() for tag in ast.literal_eval(x)])\n",
    "df[\"characteristics\"] = df[\"characteristics\"].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2afb22",
   "metadata": {},
   "source": [
    "MultiLabelBinarizer used since for each person we need to clasify multiple tags not just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = sorted(set(tag for tags in df[\"tags\"] for tag in tags))\n",
    "mlb = MultiLabelBinarizer(classes=all_tags)\n",
    "Y = mlb.fit_transform(df[\"tags\"])\n",
    "num_tags = len(all_tags)\n",
    "descriptions = df[\"characteristics\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(descriptions, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac2dd2",
   "metadata": {},
   "source": [
    "Model for embeddings and tokenizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert = AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cd099",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(texts, batch_size=16):\n",
    "    bert.eval() # Set model to evaluation mode (faster inference)\n",
    "    emb_list = []\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        # Process texts in batches to avoid memory issues\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "            outputs = bert(**inputs)\n",
    "            attention_mask = inputs['attention_mask'] # Get attention mask (which tokens are valid, which ones are padding) to handle padding\n",
    "            token_embeddings = outputs.last_hidden_state # Embeddings matrix for each token\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float() # Expand mask to match token embeddings and avoid padding issues\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) # Sum embeddings for valid tokens\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) # Count of valid tokens and avoid division by zero\n",
    "            emb = sum_embeddings / sum_mask # Average embeddings for each text\n",
    "            emb_list.append(emb)\n",
    "    return torch.cat(emb_list, dim=0) # Concatenate embeddings from all batches for all texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfffb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_train_emb = get_bert_embeddings(X_train)\n",
    "    X_test_emb = get_bert_embeddings(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba14c8",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagClassifier(nn.Module): # simple MLP for tag classification\n",
    "    def __init__(self, embed_dim, hidden_dim, num_tags):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_tags)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328fb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model = TagClassifier(X_train_emb.shape[1], 256, num_tags).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss() # Since multi-label classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "X_train_t = X_train_emb.to(device)\n",
    "Y_train_t = torch.tensor(Y_train, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4102db9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X_train_t)\n",
    "    loss = loss_fn(logits, Y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 5 == 0 or epoch == EPOCHS-1:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93acab9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X_test_t = X_test_emb.to(device)\n",
    "Y_test_t = torch.tensor(Y_test, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_t)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > 0.5).cpu().numpy() # Tensor to cpu (if it was on GPU) and convert to numpy array\n",
    "    y_true = Y_test\n",
    "    y_pred = preds\n",
    "    print(\"=== TEST SAMPLES ===\\n\")\n",
    "    for i, desc in enumerate(X_test):\n",
    "        true_tags = [t for t, f in zip(all_tags, y_true[i]) if f]\n",
    "        pred_tags = [t for t, f in zip(all_tags, y_pred[i]) if f]\n",
    "        print(f\"Text: {desc}\")\n",
    "        print(f\"True tags: {true_tags}\")\n",
    "        print(f\"Predicted: {pred_tags}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e45a74",
   "metadata": {},
   "source": [
    "## Predict/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = get_bert_embeddings([text_preprocessing(text)]).to(device)\n",
    "        logits = model(emb)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "        tags = [tag for tag, p in zip(all_tags, probs) if p > 0.5]\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EXAMPLE ===\")\n",
    "test_text = \"Experienced in data science and machine learning\"\n",
    "print(f\"Input: {test_text}\")\n",
    "print(\"Tags:\", predict_tags(test_text))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
